"""
ç«¯åˆ°ç«¯æµ‹è¯• - æµ‹è¯•å®Œæ•´è¿­ä»£å¾ªç¯å’Œé”™è¯¯æ¢å¤
"""

import pytest
import sys
import os
import asyncio
import tempfile
import json
from unittest.mock import Mock, AsyncMock, patch, MagicMock

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))


class MockBrowserTool:
    """æ¨¡æ‹Ÿæµè§ˆå™¨å·¥å…·"""

    def __init__(self, html_content: str = "<html><body>Test</body></html>"):
        self.html_content = html_content
        self.page = AsyncMock()
        self.page.url = "https://example.com"
        self.page.query_selector = AsyncMock(return_value=None)
        self.page.evaluate = AsyncMock(return_value=1000)
        self.page.wait_for_load_state = AsyncMock()
        self.page.wait_for_timeout = AsyncMock()
        self.is_started = False

    async def start(self):
        self.is_started = True

    async def stop(self):
        self.is_started = False

    async def navigate(self, url: str, **kwargs):
        """æ¨¡æ‹Ÿå¯¼èˆª"""
        pass

    async def get_html(self) -> str:
        return self.html_content

    async def take_screenshot(self, **kwargs) -> bytes:
        return b"mock_screenshot_data"

    async def scroll_to_bottom(self, delay: float = 0.5):
        pass

    async def get_current_url(self) -> str:
        return "https://example.com/page/1"

    async def wait_for_selector(self, selector: str, **kwargs) -> bool:
        return True

    async def wait_for_page_ready(self, **kwargs) -> bool:
        return True


class MockLLMClient:
    """æ¨¡æ‹ŸLLMå®¢æˆ·ç«¯"""

    def __init__(self, responses: list = None):
        self.responses = responses or ["{'strategy_type': 'css'}"]
        self.response_index = 0
        self.call_count = 0
        self.total_tokens = 0

    async def chat(self, messages: list, **kwargs) -> str:
        self.call_count += 1
        if self.response_index < len(self.responses):
            response = self.responses[self.response_index]
            self.response_index += 1
            return response
        return "{}"

    async def generate(self, prompt: str, **kwargs) -> str:
        return await self.chat([{"role": "user", "content": prompt}])

    def get_stats(self) -> dict:
        return {
            'call_count': self.call_count,
            'total_tokens': self.total_tokens,
            'model': 'mock-model',
            'provider': 'mock'
        }

    async def close(self):
        pass


# ==================== æµ‹è¯•ç”¨ä¾‹ ====================

@pytest.mark.asyncio
async def test_sense_agent_basic():
    """æµ‹è¯•æ„ŸçŸ¥æ™ºèƒ½ä½“åŸºæœ¬åŠŸèƒ½"""
    from src.agents.base import SenseAgent, DegradationTracker

    # åˆ›å»ºæ¨¡æ‹Ÿæµè§ˆå™¨
    mock_html = """
    <html>
    <body>
        <div class="product-list">
            <div class="item">
                <h3 class="title">Product 1</h3>
                <span class="price">$99.99</span>
            </div>
        </div>
        <a class="next" href="/page/2">Next</a>
    </body>
    </html>
    """
    browser = MockBrowserTool(mock_html)

    # åˆ›å»ºæ„ŸçŸ¥æ™ºèƒ½ä½“
    agent = SenseAgent()

    # æ‰§è¡Œ
    context = {
        'browser': browser,
        'spec': {'goal': 'çˆ¬å–äº§å“åˆ—è¡¨'}
    }

    result = await agent.execute(context)

    # éªŒè¯
    assert result['success'] is True
    assert 'structure' in result
    assert 'features' in result
    assert result['structure']['type'] in ['list', 'detail', 'form', 'other']
    print(f"\nâœ… æ„ŸçŸ¥æ™ºèƒ½ä½“æµ‹è¯•é€šè¿‡")
    print(f"   é¡µé¢ç±»å‹: {result['structure']['type']}")
    print(f"   åˆ†é¡µç±»å‹: {result['structure']['pagination_type']}")


@pytest.mark.asyncio
async def test_plan_agent_with_fallback():
    """æµ‹è¯•è§„åˆ’æ™ºèƒ½ä½“é™çº§ç­–ç•¥"""
    from src.agents.base import PlanAgent, DegradationTracker

    # åˆ›å»ºä¸å¸¦LLMçš„è§„åˆ’æ™ºèƒ½ä½“ï¼ˆä¼šä½¿ç”¨é™çº§ç­–ç•¥ï¼‰
    agent = PlanAgent()

    context = {
        'page_structure': {
            'type': 'list',
            'pagination_type': 'click',
            'main_content_selector': '.product-item',
            'estimated_items': 10
        },
        'spec': {
            'targets': [{
                'name': 'products',
                'fields': [
                    {'name': 'title', 'selector': '.title'},
                    {'name': 'price', 'selector': '.price'}
                ]
            }]
        }
    }

    result = await agent.execute(context)

    # éªŒè¯
    assert result['success'] is True
    assert 'strategy' in result
    assert 'selectors' in result
    assert 'generated_code' in result
    assert '.title' in result['selectors'] or 'title' in result['selectors']
    print(f"\nâœ… è§„åˆ’æ™ºèƒ½ä½“é™çº§ç­–ç•¥æµ‹è¯•é€šè¿‡")
    print(f"   ç­–ç•¥ç±»å‹: {result['strategy']['strategy_type']}")
    print(f"   é€‰æ‹©å™¨æ•°é‡: {len(result['selectors'])}")


@pytest.mark.asyncio
async def test_act_agent_extraction_metrics():
    """æµ‹è¯•æ‰§è¡Œæ™ºèƒ½ä½“æå–æŒ‡æ ‡"""
    from src.agents.base import ActAgent, ExtractionMetrics

    # åˆ›å»ºæ¨¡æ‹Ÿæµè§ˆå™¨
    mock_html = """
    <html>
    <body>
        <div class="product-item">
            <h3 class="title">Product 1</h3>
            <span class="price">$99.99</span>
        </div>
        <div class="product-item">
            <h3 class="title">Product 2</h3>
            <span class="price">$199.99</span>
        </div>
        <div class="product-item">
            <h3 class="title">Product 3</h3>
            <span class="price"></span>
        </div>
    </body>
    </html>
    """
    browser = MockBrowserTool(mock_html)

    # åˆ›å»ºæ‰§è¡Œæ™ºèƒ½ä½“
    agent = ActAgent()

    context = {
        'browser': browser,
        'selectors': {
            'title': '.title',
            'price': '.price'
        },
        'strategy': {
            'container_selector': '.product-item',
            'pagination_strategy': 'none'
        },
        'spec': {
            'targets': [{
                'name': 'products',
                'fields': [
                    {'name': 'title', 'selector': '.title', 'required': True},
                    {'name': 'price', 'selector': '.price', 'required': False}
                ]
            }]
        }
    }

    result = await agent.execute(context)

    # éªŒè¯
    assert result['success'] is True
    assert len(result['extracted_data']) == 3
    assert 'extraction_metrics' in result

    metrics = result['extraction_metrics']
    assert 'total_items' in metrics
    assert 'failed_selectors' in metrics
    print(f"\nâœ… æ‰§è¡Œæ™ºèƒ½ä½“æå–æŒ‡æ ‡æµ‹è¯•é€šè¿‡")
    print(f"   æå–æ•°é‡: {result['count']}")
    print(f"   æˆåŠŸç‡: {metrics['success_rate']:.1%}")


@pytest.mark.asyncio
async def test_verify_agent_quality_check():
    """æµ‹è¯•éªŒè¯æ™ºèƒ½ä½“è´¨é‡æ£€æŸ¥"""
    from src.agents.base import VerifyAgent

    agent = VerifyAgent()

    # æµ‹è¯•æ•°æ®
    context = {
        'extracted_data': [
            {'title': 'Product 1', 'price': '$99.99'},
            {'title': 'Product 2', 'price': '$199.99'},
            {'title': '', 'price': '$299.99'},  # ç¼ºå°‘å¿…å¡«å­—æ®µ
            {'title': 'Product 4', 'price': ''},  # ä»·æ ¼ä¸ºç©º
        ],
        'spec': {
            'targets': [{
                'name': 'products',
                'fields': [
                    {'name': 'title', 'required': True},
                    {'name': 'price', 'required': False}
                ]
            }]
        },
        'extraction_metrics': {
            'total_items': 4,
            'missing_fields': {'title': 1},
            'empty_fields': {'price': 1}
        }
    }

    result = await agent.execute(context)

    # éªŒè¯
    assert result['success'] is True
    assert 'quality_score' in result
    assert result['quality_score'] >= 0 and result['quality_score'] <= 1
    assert 'verification_result' in result
    print(f"\nâœ… éªŒè¯æ™ºèƒ½ä½“æµ‹è¯•é€šè¿‡")
    print(f"   è´¨é‡åˆ†æ•°: {result['quality_score']:.2f}")
    print(f"   æœ‰æ•ˆæ•°æ®: {result['valid_items']}/{result['total_items']}")


@pytest.mark.asyncio
async def test_judge_agent_decision():
    """æµ‹è¯•å†³ç­–æ™ºèƒ½ä½“å†³ç­–é€»è¾‘"""
    from src.agents.base import JudgeAgent

    agent = JudgeAgent()

    # æµ‹è¯•åœºæ™¯1: è´¨é‡åˆ†æ•°é«˜ï¼Œåº”è¯¥å®Œæˆ
    context = {
        'quality_score': 0.9,
        'iteration': 0,
        'max_iterations': 10,
        'errors': [],
        'spec': {},
        'extracted_data': [{'title': 'Product 1'} for _ in range(10)]
    }

    result = await agent.execute(context)

    assert result['success'] is True
    assert result['decision'] == 'complete'
    assert 'reasoning' in result
    print(f"\nâœ… å†³ç­–æ™ºèƒ½ä½“æµ‹è¯•é€šè¿‡")
    print(f"   å†³ç­–: {result['decision']}")
    print(f"   åŸå› : {result['reasoning']}")

    # æµ‹è¯•åœºæ™¯2: è´¨é‡åˆ†æ•°ä¸­ç­‰ï¼Œåº”è¯¥ç»§ç»­è¿­ä»£
    context2 = {
        'quality_score': 0.5,
        'iteration': 2,
        'max_iterations': 10,
        'errors': ['selector_error'],
        'spec': {},
        'extracted_data': [{'title': 'Product 1'} for _ in range(3)]
    }

    result2 = await agent.execute(context2)
    assert result2['decision'] == 'reflect_and_retry'
    print(f"   å†³ç­–(ä¸­ç­‰è´¨é‡): {result2['decision']}")


@pytest.mark.asyncio
async def test_degradation_tracker():
    """æµ‹è¯•é™çº§è¿½è¸ªå™¨"""
    from src.agents.base import DegradationTracker

    tracker = DegradationTracker(warning_threshold=2)

    # è®°å½•ç¬¬ä¸€æ¬¡é™çº§
    info1 = tracker.record_degradation('SenseAgent', 'llm_analyze', 'Timeout')
    assert info1['is_degraded'] is True
    assert info1['should_warn'] is False

    # è®°å½•ç¬¬äºŒæ¬¡é™çº§
    info2 = tracker.record_degradation('PlanAgent', 'generate_strategy', 'API Error')
    assert info2['should_warn'] is True  # è¾¾åˆ°é˜ˆå€¼

    # è·å–ç»Ÿè®¡
    stats = tracker.get_stats()
    assert stats['total_degradations'] == 2
    assert len(stats['history']) == 2

    print(f"\nâœ… é™çº§è¿½è¸ªå™¨æµ‹è¯•é€šè¿‡")
    print(f"   æ€»é™çº§æ¬¡æ•°: {stats['total_degradations']}")
    print(f"   è­¦å‘Šé˜ˆå€¼: {stats['warning_threshold']}")


@pytest.mark.asyncio
async def test_browser_retry_mechanism():
    """æµ‹è¯•æµè§ˆå™¨é‡è¯•æœºåˆ¶"""
    from src.tools.browser import BrowserTool, with_retry

    # æ¨¡æ‹Ÿé‡è¯•åœºæ™¯
    call_count = 0

    class FailingBrowser:
        async def failing_operation(self):
            nonlocal call_count
            call_count += 1
            if call_count < 3:
                from playwright.async_api import Error
                raise Error("Navigation failed")
            return "success"

    # æµ‹è¯•é‡è¯•è£…é¥°å™¨
    browser = FailingBrowser()

    # åº”ç”¨è£…é¥°å™¨
    @with_retry(max_retries=3, base_delay=0.1, max_delay=1.0)
    async def operation(self):
        return await self.failing_operation()

    result = await operation(browser)
    assert result == "success"
    assert call_count == 3  # å¤±è´¥2æ¬¡ï¼ŒæˆåŠŸ1æ¬¡

    print(f"\nâœ… æµè§ˆå™¨é‡è¯•æœºåˆ¶æµ‹è¯•é€šè¿‡")
    print(f"   é‡è¯•æ¬¡æ•°: {call_count - 1}")


@pytest.mark.asyncio
async def test_state_manager_concurrency():
    """æµ‹è¯•çŠ¶æ€ç®¡ç†å™¨å¹¶å‘å®‰å…¨"""
    from src.core.state_manager import StateManager

    manager = StateManager()
    await manager.create_initial_state('test_task', {'goal': 'test'})

    # å¹¶å‘æ›´æ–°
    async def update_state(i):
        await manager.update_state({'iteration': i}, f"update_{i}")

    # å¹¶å‘æ‰§è¡Œ100æ¬¡æ›´æ–°
    tasks = [update_state(i) for i in range(100)]
    await asyncio.gather(*tasks)

    # éªŒè¯æ›´æ–°è®¡æ•°
    assert manager.get_update_count() == 100

    print(f"\nâœ… çŠ¶æ€ç®¡ç†å™¨å¹¶å‘æµ‹è¯•é€šè¿‡")
    print(f"   æ›´æ–°æ¬¡æ•°: {manager.get_update_count()}")


@pytest.mark.asyncio
async def test_llm_retry_mechanism():
    """æµ‹è¯•LLMå®¢æˆ·ç«¯é‡è¯•æœºåˆ¶"""
    from src.tools.llm_client import LLMClient, LLMException, ErrorType

    # åˆ›å»ºæ¨¡æ‹Ÿå®¢æˆ·ç«¯
    client = LLMClient(api_key='test_key', model='test_model')

    # æ¨¡æ‹ŸAPIè°ƒç”¨
    call_count = 0

    async def mock_post(*args, **kwargs):
        nonlocal call_count
        call_count += 1
        mock_response = Mock()
        if call_count < 3:
            mock_response.status_code = 500
            mock_response.text = "Internal Server Error"
        else:
            mock_response.status_code = 200
            mock_response.json = lambda: {
                'choices': [{'message': {'content': 'success'}}],
                'usage': {'total_tokens': 100}
            }
        return mock_response

    client.client.post = mock_post

    # æµ‹è¯•é‡è¯•
    result = await client.generate("test prompt", max_retries=3)

    assert call_count == 3
    assert result == 'success'
    assert client.retry_count == 2

    print(f"\nâœ… LLMé‡è¯•æœºåˆ¶æµ‹è¯•é€šè¿‡")
    print(f"   APIè°ƒç”¨æ¬¡æ•°: {call_count}")
    print(f"   é‡è¯•æ¬¡æ•°: {client.retry_count}")


@pytest.mark.asyncio
async def test_full_iteration_loop():
    """æµ‹è¯•å®Œæ•´è¿­ä»£å¾ªç¯"""
    from src.agents.base import AgentPool, AgentCapability

    # åˆ›å»ºæ¨¡æ‹Ÿæµè§ˆå™¨
    mock_html = """
    <html>
    <body>
        <div class="product-item">
            <h3 class="title">Product 1</h3>
            <span class="price">$99.99</span>
        </div>
        <div class="product-item">
            <h3 class="title">Product 2</h3>
            <span class="price">$199.99</span>
        </div>
    </body>
    </html>
    """
    browser = MockBrowserTool(mock_html)

    # åˆ›å»ºæ™ºèƒ½ä½“æ± 
    pool = AgentPool()

    # 1. æ„ŸçŸ¥
    sense_result = await pool.execute_capability(
        AgentCapability.SENSE,
        {'browser': browser, 'spec': {'goal': 'çˆ¬å–äº§å“'}}
    )
    assert sense_result['success']

    # 2. è§„åˆ’
    plan_result = await pool.execute_capability(
        AgentCapability.PLAN,
        {
            'page_structure': sense_result['structure'],
            'spec': {
                'targets': [{
                    'name': 'products',
                    'fields': [
                        {'name': 'title', 'selector': '.title'},
                        {'name': 'price', 'selector': '.price'}
                    ]
                }]
            }
        }
    )
    assert plan_result['success']

    # 3. æ‰§è¡Œ
    act_result = await pool.execute_capability(
        AgentCapability.ACT,
        {
            'browser': browser,
            'selectors': plan_result['selectors'],
            'strategy': plan_result['strategy']
        }
    )
    assert act_result['success']

    # 4. éªŒè¯
    verify_result = await pool.execute_capability(
        AgentCapability.VERIFY,
        {
            'extracted_data': act_result['extracted_data'],
            'spec': {'targets': [{'name': 'products', 'fields': []}]}
        }
    )
    assert verify_result['success']

    # 5. å†³ç­–
    judge_result = await pool.execute_capability(
        AgentCapability.JUDGE,
        {
            'quality_score': verify_result['quality_score'],
            'iteration': 0,
            'max_iterations': 5,
            'errors': [],
            'extracted_data': act_result['extracted_data']
        }
    )
    assert judge_result['success']
    assert judge_result['decision'] in ['complete', 'reflect_and_retry', 'terminate']

    print(f"\nâœ… å®Œæ•´è¿­ä»£å¾ªç¯æµ‹è¯•é€šè¿‡")
    print(f"   æ„ŸçŸ¥æˆåŠŸ: {sense_result['success']}")
    print(f"   è§„åˆ’æˆåŠŸ: {plan_result['success']}")
    print(f"   æå–æ•°é‡: {act_result['count']}")
    print(f"   è´¨é‡åˆ†æ•°: {verify_result['quality_score']:.2f}")
    print(f"   æœ€ç»ˆå†³ç­–: {judge_result['decision']}")


@pytest.mark.asyncio
async def test_error_recovery_path():
    """æµ‹è¯•é”™è¯¯æ¢å¤è·¯å¾„"""
    from src.agents.base import AgentPool, AgentCapability

    # åˆ›å»ºæ™ºèƒ½ä½“æ± 
    pool = AgentPool()

    # æ¨¡æ‹Ÿé”™è¯¯åœºæ™¯ï¼šcan_handle è¿”å› False
    context = {'browser': None}

    # SenseAgent çš„ can_handle ä¼šè¿”å› False (browser is None)
    agent = pool.get_agent(AgentCapability.SENSE)
    assert agent.can_handle(context) is False, "can_handle should return False when browser is None"

    # æ‰§è¡Œä¼šè¿”å›é”™è¯¯ï¼ˆå› ä¸º can_handle è¿”å› Falseï¼‰
    sense_result = await pool.execute_capability(
        AgentCapability.SENSE,
        context
    )

    # åº”è¯¥è¿”å›å¤±è´¥
    assert sense_result['success'] is False
    assert 'error' in sense_result

    # æ¨¡æ‹Ÿåå°„æ™ºèƒ½ä½“å¤„ç†é”™è¯¯
    reflect_result = await pool.execute_capability(
        AgentCapability.REFLECT,
        {
            'execution_history': [{'stage': 'sense', 'error': 'browser not initialized'}],
            'errors': ['browser_error: Browser is None'],
            'quality_score': 0,
            'spec': {}
        }
    )

    assert reflect_result['success']
    assert 'improvements' in reflect_result
    assert 'suggested_action' in reflect_result

    print(f"\nâœ… é”™è¯¯æ¢å¤è·¯å¾„æµ‹è¯•é€šè¿‡")
    print(f"   é”™è¯¯æ£€æµ‹: {sense_result['error']}")
    print(f"   æ¢å¤å»ºè®®: {reflect_result['suggested_action']}")


# ==================== ä¸»å‡½æ•° ====================

if __name__ == '__main__':
    print("\n" + "=" * 60)
    print("ğŸš€ è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•")
    print("=" * 60)

    asyncio.run(test_sense_agent_basic())
    asyncio.run(test_plan_agent_with_fallback())
    asyncio.run(test_act_agent_extraction_metrics())
    asyncio.run(test_verify_agent_quality_check())
    asyncio.run(test_judge_agent_decision())
    asyncio.run(test_degradation_tracker())
    asyncio.run(test_browser_retry_mechanism())
    asyncio.run(test_state_manager_concurrency())
    asyncio.run(test_llm_retry_mechanism())
    asyncio.run(test_full_iteration_loop())
    asyncio.run(test_error_recovery_path())

    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰ç«¯åˆ°ç«¯æµ‹è¯•é€šè¿‡!")
    print("=" * 60 + "\n")